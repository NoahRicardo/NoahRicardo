<!DOCTYPE html>

<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
  
  <title>生产者api示例 [ NoahRicardo ]</title>
  
    <!-- stylesheets list from config.yml -->
    
      <link rel="stylesheet" href="../../../../../css/iLiKE.css">
    
  
  
  
  <script src="//cdn1.lncld.net/static/js/3.1.1/av-min.js"></script>
    <script id="leancloud">
      AV.init({
          appId: "6E5zTbTljdUbVW2WkXPsXGJk-gzGzoHsz",
          appKey: "0vsyDKfNpeSECAI70J794ugv"
      });
    </script>

<meta name="generator" content="Hexo 5.4.2"></head>
<body>
    <div class="header">
        <div class="container">
    <div class="menu">
      <div class="menu-left">
        <a href="/">
          <img src="../../../../../favicon.ico"></img>
        </a>
      </div>
      <div class="menu-right">
        
          
          
          
          
          
          
          <a href="/">首页</a>
        
          
          
          
          
          
          
          <a href="/archives">归档</a>
        
          
          
          
          
          
          
          <a href="/about">关于</a>
        
      </div>
    </div>
</div>
    </div>
    <div class="container">
        <h1 class="post-title">生产者api示例</h1>
<article class="post markdown-style">
  <h2 id="三、生产者api示例"><a href="#三、生产者api示例" class="headerlink" title="三、生产者api示例"></a>三、生产者api示例</h2><p>一个正常的生产逻辑需要具备以下几个步骤</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(1)配置生产者客户端参数及创建相应的生产者实例</span><br><span class="line">(2)构建待发送的消息</span><br><span class="line">(3)发送消息</span><br><span class="line">(4)关闭生产者实例</span><br></pre></td></tr></table></figure>

<ul>
<li>示例代码（部分截取）</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>(); </span><br><span class="line"></span><br><span class="line"><span class="comment">//设置 kafka 集群的地址</span></span><br><span class="line">props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;master:9092,slave1:9092,slave2:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//ack 模式,取值有 0,1,-1(all) , all 是最慢但最安全的，</span></span><br><span class="line">props.put(“acks”, “all”); </span><br><span class="line"></span><br><span class="line"><span class="comment">//失败重试次数-&gt;失败会自动重试（可恢复/不可恢复）--&gt;(有可能会造成数据的乱序)</span></span><br><span class="line">props.put(“retries”, <span class="number">3</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">//数据发送的批次大小提高效率/吞吐量太大会数据延迟</span></span><br><span class="line">props.put(“batch.size”, <span class="number">10</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">//消息在缓冲区保留的时间,超过设置的值就会被提交到服务端</span></span><br><span class="line">props.put(<span class="string">&quot;linger.ms&quot;</span>, <span class="number">10000</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">//数据发送请求的最大缓存数</span></span><br><span class="line">props.put(<span class="string">&quot;max.request.size&quot;</span>,<span class="number">10</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">//整个 Producer 用到总内存的大小,如果缓冲区满了会提交数据到服务端 buffer.memory 要大于 batch.size,否则会报申请内存不足的错误降低阻塞的可能性</span></span><br><span class="line">props.put(“buffer.memory”, <span class="number">10240</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//key-value序列化器</span></span><br><span class="line">props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">//字符串最好</span></span><br><span class="line">props.put(“value.serializer”, “org.apache.kafka.common.serialization.StringSerializer”);</span><br></pre></td></tr></table></figure>

<ul>
<li>消息对象 ProducerRecord,它并不是单纯意义上的消息,它包含了多个属性,原本需要发送的与业务关的消息体只是其中的一个 value 属性 ,比“ Hello, rgzn!”只是 ProducerRecord 对象的一个属性。</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ProducerRecord 类的定义如下:</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ProducerRecord</span>&lt;K, V&gt; &#123; </span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> String topic; </span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> Integer partition;</span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> Headers headers; </span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> K key; </span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> V value; </span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> Long timestamp;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="3-2-必要参数配置"><a href="#3-2-必要参数配置" class="headerlink" title="3.2 必要参数配置"></a>3.2 必要参数配置</h5><ul>
<li>在创建真正的生产者实例前需要配置相应的参数,比如需要连接的 Kafka 集群地址。在 Kafka 生产者客户端 KatkaProducer 中有 3 个参数是必填的。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">* bootstrap.servers </span><br><span class="line">* key.serializer </span><br><span class="line">* value.serializer</span><br></pre></td></tr></table></figure>

<ul>
<li>为了防止参数名字符串书写错误,可以使用如下方式进行设置: </li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">props.setProperty(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG,ProducerInterceptorPrefix.class.getName());</span><br><span class="line">props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,<span class="string">&quot;master:9092,slave1:9092&quot;</span>); </span><br><span class="line">props.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,StringSerializer.class.getName()); props.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,StringSerializer.class.getName());</span><br></pre></td></tr></table></figure>

<h5 id="3-3-生产者api参数发送方式"><a href="#3-3-生产者api参数发送方式" class="headerlink" title="3.3  生产者api参数发送方式"></a>3.3  生产者api参数发送方式</h5><p>这个客户端经过了生产环境测试并且通常情况它比原来Scals客户端更加快速、功能更加齐全。你可以通过添加以下示例的Maven坐标到客户端依赖中来使用这个新的客户端（你可以修改版本号来使用新的发布版本）：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>0.10.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>



<h6 id="3-3-1-发后即忘-fire-and-forget"><a href="#3-3-1-发后即忘-fire-and-forget" class="headerlink" title="3.3.1  发后即忘( fire-and-forget)"></a>3.3.1  发后即忘( fire-and-forget)</h6><p>发后即忘,它只管往 Kafka 发送,并不关心消息是否正确到达。在大多数情况下,这种发送方式没有问题; 不过在某些时候(比如发生不可重试异常时)会造成消息的丢失。这种发送方式的性能最高,可靠性最差。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Future&lt;RecordMetadata&gt; send = producer.send(rcd);</span><br></pre></td></tr></table></figure>



<h6 id="3-3-2-同步发送-sync"><a href="#3-3-2-同步发送-sync" class="headerlink" title="3.3.2  同步发送(sync )"></a>3.3.2  同步发送(sync )</h6><p>0.8.x 前,有一个参数 producer.type=sycn|asycn 来决定生产者的发送模式;现已失效(新版中,producer 在底层只有异步)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">try &#123;</span><br><span class="line">	producer.send(rcd).get(); </span><br><span class="line">&#125; catch (Exception e) &#123; </span><br><span class="line">	e.printStackTrace();</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<p>在调用 <code>send</code> 方法后可以接着调用 <code>get()</code> 方法，<code>send</code> 方法的返回值是一个 Future\对象，RecordMetadata 里面包含了发送消息的主题、分区、偏移量等信息。改写后的代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        ProducerRecord&lt;String, String&gt; record = <span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(topicName, <span class="string">&quot;k&quot;</span> + i, <span class="string">&quot;world&quot;</span> + i);</span><br><span class="line">        <span class="comment">/*同步发送消息*/</span></span><br><span class="line">        <span class="type">RecordMetadata</span> <span class="variable">metadata</span> <span class="operator">=</span> producer.send(record).get();</span><br><span class="line">        System.out.printf(<span class="string">&quot;topic=%s, partition=%d, offset=%s \n&quot;</span>,</span><br><span class="line">                metadata.topic(), metadata.partition(), metadata.offset());</span><br><span class="line">    &#125; <span class="keyword">catch</span> (InterruptedException | ExecutionException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>此时得到的输出如下：偏移量和调用次数有关，所有记录都分配到了 0 分区，这是因为在创建 <code>Hello-Kafka</code> 主题时候，使用 <code>--partitions</code> 指定其分区数为 1，即只有一个分区。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">topic=Hello-Kafka, partition=0, offset=40 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=41 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=42 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=43 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=44 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=45 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=46 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=47 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=48 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=49</span><br></pre></td></tr></table></figure>



<h6 id="3-3-3-异步发送-async"><a href="#3-3-3-异步发送-async" class="headerlink" title="3.3.3 异步发送(async )"></a>3.3.3 异步发送(async )</h6><p>回调函数会在 producer 收到 ack 时调用,为异步调用,该方法有两个参数,分别是 RecordMetadata 和Exception,如果 Exception 为 null,说明消息发送成功,如果 Exception 不为 null,说明消息发送失败。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意:消息发送失败会自动重试,不需要我们在回调函数中手动重试</span><br></pre></td></tr></table></figure>

<p>通常我们并不关心发送成功的情况，更多关注的是失败的情况，因此 Kafka 提供了异步发送和回调函数。 代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">    ProducerRecord&lt;String, String&gt; record = <span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(topicName, <span class="string">&quot;k&quot;</span> + i, <span class="string">&quot;world&quot;</span> + i);</span><br><span class="line">    <span class="comment">/*异步发送消息，并监听回调*/</span></span><br><span class="line">    producer.send(record, <span class="keyword">new</span> <span class="title class_">Callback</span>() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onCompletion</span><span class="params">(RecordMetadata metadata, Exception exception)</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (exception != <span class="literal">null</span>) &#123;</span><br><span class="line">                System.out.println(<span class="string">&quot;进行异常处理&quot;</span>);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                System.out.printf(<span class="string">&quot;topic=%s, partition=%d, offset=%s \n&quot;</span>,</span><br><span class="line">                        metadata.topic(), metadata.partition(), metadata.offset());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h5 id="3-4-生产者原理解析"><a href="#3-4-生产者原理解析" class="headerlink" title="3.4 生产者原理解析"></a>3.4 生产者原理解析</h5><p><img src="D:%E5%AD%A6%E4%B9%A0%C2%822%E5%AD%A6%E6%9C%9Fhadoopimgs%5Cimage-20220514225146784.png" alt="image-20220514225146784"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">0、新建kafka生产实例，参数也是放在kafkaProducer里面</span><br><span class="line"></span><br><span class="line">1、Producerinterceptor拦截器，设置特定的规则对消息进行拦截，可以通过指定的消息</span><br><span class="line"></span><br><span class="line">2.Serializer序列化器，创建生产者对象时必须指定序列化器，作用就是将key和value转换为二进制</span><br><span class="line"></span><br><span class="line">3.Partitioner，topic中有分区，如何分发就是通过此处有规划的分发数据</span><br><span class="line"></span><br><span class="line">4.1 RecordAccumulator消息累加器，其中有多个分区，对于每个分区，都会单独维护主要用来缓存消息以便 Sender 线程可以批量发送, 进而减少网络传输的资源消耗以提升性能。</span><br><span class="line"></span><br><span class="line">4.2 RecordAccumulator 缓存的大小可以通过生产者客户端参数 buffer.memory 配 置, 默认值为 33554432B ,即 32M。</span><br><span class="line"></span><br><span class="line">4.3 如果生产者发送消息的速度超过发送到服务器的速度,则会导致生产者空间不足,这个时KafkaProducer.send()方法调用要么被阻塞,要么抛出异常,这个取决于参数max.block.ms 的配置,此参数的默认值为 60000,即 60 秒。（此配置可以理解为阻塞时间，在这个范围内不会抛出异常）</span><br><span class="line"></span><br><span class="line">4.4主线程中发送过来的消息都会被迫加到 RecordAccumulator 的某个双端队列(Deque )中,RecordAccumulator 内部为每个分区都维护了一个双端队列,即Deque&lt;ProducerBatch&gt;。消息写入缓存时,追加到双端队列的尾部;</span><br><span class="line"></span><br><span class="line">4.5、 Sender 读取消息时,从双端队列的头部读取。</span><br><span class="line"></span><br><span class="line">4.6 注意:ProducerBatch 是指一个消息批次; 与此同时,会将较小的 ProducerBatch凑成一个较ProducerBatch ,也可以减少网络请求的次数以提升整体的吞吐量。</span><br><span class="line"></span><br><span class="line">4.7 ProducerBatch 大小和 batch.size 参数也有着密切的关系。</span><br><span class="line"></span><br><span class="line">4.8 当一条消息(ProducerRecord ) 流入RecordAccumulator 时,会先寻找与消息分区所对应的双端队列(如果没有则新建),再从这个双端队列的尾部获取一个ProducerBatch (如果没有则新建),查看 ProducerBatch 中是否还可以写入这个ProducerRecord,如果可以写入,如果不可以则需要创建一个新的 Producer Batch。</span><br><span class="line"></span><br><span class="line">4.9 在新建ProducerBatch 时评估这条消息的大小是否超过 batch.size 参数大小, 如果不超过, 那么就以 batch.size 参数的大小来创建 ProducerBatch。</span><br><span class="line"></span><br><span class="line">4.10 如果生产者客户端需要向很多分区发送消息, 则可以将 buffer.memory 参数适当调大以增加整体的吞吐量</span><br><span class="line">6.1、 Sender 从 RecordAccumulator 获取缓存的消息之后,会进一步将&lt;分区,Deque&lt;Producer Batch&gt;&gt;的形式转变成&lt;Node,List&lt;ProducerBatch&gt;的形式,其中 Node 表示 Kafka 集群 broker 节点。</span><br><span class="line"></span><br><span class="line">6.2 对于网络连接来说,生产者客户端是与具体 broker 节点建立的连接,也就是向具体的 broker 节点发送消息,而并不关心消息属于哪一个分区;</span><br><span class="line"></span><br><span class="line">6.3 、 而对于 KafkaProducer 的应用逻辑而言,我们只关注向哪个分区中发送哪些消息,所以在这里需要做一个应用逻辑层面到网络 I/O层面的转换。</span><br><span class="line"></span><br><span class="line">6.4 在转换成&lt;Node, List&lt;ProducerBatch&gt;&gt;的形式之后, Sender 会进一步封装成&lt;Node,Request&gt; 的形式, 这样就可以将 Request 请求发往各个 Node 了,这里的 Request 是 Kafka 各种协议请求;</span><br><span class="line"></span><br><span class="line">6.5 下一步直接就可以从Request 发送到 Selector 在转到 kafka集群</span><br><span class="line"></span><br><span class="line">7.1 缓存操作可以理解为当请求 从 sender 发送给 kafka 集群时候，sender 是不知道是否成功发送，即kafka 是否接收到消息，所以此功能是，当sender只要给 kafka 发送请求，此消息就同步InFlightRequests</span><br><span class="line"></span><br><span class="line">7.2 请求在从 sender 线程发往 Kafka 之前还会保存到InFlightRequests 中,InFlightRequests 保存对象的具体形式为Map&lt;Nodeld, Deque&lt;request&gt;&gt;,它的主要作用是缓存了已经发出去但还没有收到服务端响应的请求(Nodeld 是一个 String 类型,表示节点的 id 编号)。</span><br><span class="line"></span><br><span class="line">7.3 与此同时,InFlightRequests 还提供了许多管理类的方法,并且通过配置参数还可以限制每个连接(也就是客户端与 Node 之间的连接) 最多缓存的请求数。</span><br><span class="line"></span><br><span class="line">7.4 这个配置参数为 max.in.flight.request.per. connection ,默认值为 5,即每个连接最多只能缓存 5 个未响应的请求,超过该数值之后就不能再向这个连接发送更多的请求了,除非有缓存的请求收到了响应</span><br><span class="line">( Response )。</span><br><span class="line"></span><br><span class="line">8.1 提交到selector 准备发送</span><br><span class="line"></span><br><span class="line">9、发送到 kafka集群</span><br><span class="line"></span><br><span class="line">10、当 kafka 集群受到消息 ，集群响应，返回给selector</span><br><span class="line"></span><br><span class="line">11.1、selector 回复给 InFlightRequests</span><br><span class="line"></span><br><span class="line">11.2、如果没有受到响应， request 则会在 InFlightRequests 一直缓存</span><br><span class="line">11.3 通过比较 Deque&lt;Request&gt; 的 size 与这个参数的大小来判断对应的 Node中是否己经堆积了很多未响应的消息, 如果真是如此, 那么说明这个 Node 节点负载较大或网络连接有问题,再继其发送请求会增大请求超时的可能。</span><br></pre></td></tr></table></figure>

<h4 id="4、消费者API"><a href="#4、消费者API" class="headerlink" title="4、消费者API"></a>4、消费者API</h4><p>一个正常的消费逻辑需要具备以下几个步骤: </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(1)配置消费者客户端参数</span><br><span class="line">(2)创建相应的消费者实例; </span><br><span class="line">(3)订阅主题; </span><br><span class="line">(4)拉取消息并消费; </span><br><span class="line">(5)提交消费位移 offset;</span><br><span class="line">(6)关闭消费者实例。</span><br></pre></td></tr></table></figure>

<ul>
<li>消费者API示例代码（部分截取）</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>(); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义 kakfa 服务的地址,不需要将所有 broker 指定上</span></span><br><span class="line">props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;master:9092&quot;</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定 consumer group </span></span><br><span class="line">props.put(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;g1&quot;</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 是否自动提交 offset </span></span><br><span class="line">props.put(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;true&quot;</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 自动提交 offset 的时间间隔</span></span><br><span class="line">props.put(<span class="string">&quot;auto.commit.interval.ms&quot;</span>, <span class="string">&quot;1000&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// key 的反序列化类</span></span><br><span class="line">props.put(<span class="string">&quot;key.deserializer&quot;</span>,<span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">// value 的反序列化类</span></span><br><span class="line">props.put(<span class="string">&quot;value.deserializer&quot;</span>,<span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 如果没有消费偏移量记录,则自动重设为起始 offset:latest, earliest, none</span></span><br><span class="line"><span class="comment">//Earliest目前状态下最前面的一条消息（日志在一定保存时间后会自动清空）</span></span><br><span class="line"><span class="comment">//none（上次记录的偏移量，如果没有，会抛异常） </span></span><br><span class="line">props.put(<span class="string">&quot;auto.offset.reset&quot;</span>,<span class="string">&quot;earliest&quot;</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义 consumer KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); </span></span><br><span class="line"><span class="comment">// 消费者订阅的 topic, 可同时订阅多个</span></span><br><span class="line">consumer.subscribe(Arrays.asList(<span class="string">&quot;first&quot;</span>, <span class="string">&quot;test&quot;</span>,<span class="string">&quot;test1&quot;</span>)); </span><br></pre></td></tr></table></figure>

<h5 id="4-1-Kafka消费者可选属性"><a href="#4-1-Kafka消费者可选属性" class="headerlink" title="4.1 Kafka消费者可选属性"></a>4.1 Kafka消费者可选属性</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">1. fetch.min.byte</span><br><span class="line">消费者从服务器获取记录的最小字节数。如果可用的数据量小于设置值，broker 会等待有足够的可用数据时才会把它返回给消费者。</span><br><span class="line"></span><br><span class="line">2. fetch.max.wait.ms</span><br><span class="line">broker 返回给消费者数据的等待时间，默认是 500ms。</span><br><span class="line"></span><br><span class="line">3. max.partition.fetch.bytes</span><br><span class="line">该属性指定了服务器从每个分区返回给消费者的最大字节数，默认为 1MB。</span><br><span class="line"></span><br><span class="line">4. session.timeout.ms</span><br><span class="line">消费者在被认为死亡之前可以与服务器断开连接的时间，默认是 3s。</span><br><span class="line"></span><br><span class="line">5. auto.offset.reset</span><br><span class="line">该属性指定了消费者在读取一个没有偏移量的分区或者偏移量无效的情况下该作何处理：</span><br><span class="line"></span><br><span class="line">latest (默认值) ：在偏移量无效的情况下，消费者将从最新的记录开始读取数据（在消费者启动之后生成的最新记录）;</span><br><span class="line">earliest ：在偏移量无效的情况下，消费者将从起始位置读取分区的记录。</span><br><span class="line">6. enable.auto.commit</span><br><span class="line">是否自动提交偏移量，默认值是 true。为了避免出现重复消费和数据丢失，可以把它设置为 false。</span><br><span class="line"></span><br><span class="line">7. client.id</span><br><span class="line">客户端 id，服务器用来识别消息的来源。</span><br><span class="line"></span><br><span class="line">8. max.poll.records</span><br><span class="line">单次调用 poll() 方法能够返回的记录数量。</span><br><span class="line"></span><br><span class="line">9. receive.buffer.bytes &amp; send.buffer.byte</span><br><span class="line">这两个参数分别指定 TCP socket 接收和发送数据包缓冲区的大小，-1 代表使用操作系统的默认值。</span><br></pre></td></tr></table></figure>

<ul>
<li>必要参数配置</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>(); props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,StringDeserializer.class.getName());</span><br><span class="line"></span><br><span class="line">props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,StringDeserializer.class.getName());</span><br><span class="line"></span><br><span class="line">props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,brokerList);</span><br><span class="line"></span><br><span class="line">props.put(ConsumerConfig.GROUP_ID_CONFIG,groupid);</span><br><span class="line"></span><br><span class="line">props.put(ConsumerConfig.CLIENT_ID_CONFIG,clientid);</span><br></pre></td></tr></table></figure>



<h5 id="4-2-subscribe-订阅主题"><a href="#4-2-subscribe-订阅主题" class="headerlink" title="4.2 subscribe 订阅主题"></a>4.2 subscribe 订阅主题</h5><ul>
<li><p>subscribe 有如下重载方法: </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">subscribe</span><span class="params">(Collection&lt;String&gt; topics,ConsumerRebalanceListener listener)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">subscribe</span><span class="params">(Collection&lt;String&gt; topics)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">subscribe</span><span class="params">(Pattern pattern, ConsumerRebalanceListener listener)</span> </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">subscribe</span><span class="params">(Pattern pattern)</span></span><br></pre></td></tr></table></figure></li>
<li><p>指定集合方式订阅主题</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">consumer.subscribe(Arrays.asList(topic1)); </span><br><span class="line">consumer <span class="title function_">subscribe</span><span class="params">(Arrays.asList(topic2)</span>);</span><br></pre></td></tr></table></figure></li>
<li><p>正则方式订阅主题</p>
<p>如果消费者采用的是正则表达式的方式(subscribe(Pattern))订阅, 在之后的过程中,如果有人又创建了新的主题,并且主题名字与正表达式相匹配,那么这个消费者就可以消费到新添加的主题中的消息。如果应用程序需要消费多个主题,并且可以处理不同的类型,那么这种订阅方式就很有效。</p>
<ul>
<li><p>正则表达式的方式订阅的示例如下</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">consumer.subscribe(Pattern.compile (<span class="string">&quot;topic.*&quot;</span> )); </span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h5 id="4-3-assign-订阅主题"><a href="#4-3-assign-订阅主题" class="headerlink" title="4.3 assign 订阅主题"></a>4.3 assign 订阅主题</h5><p>消费者不仅可以通过 KafkaConsumer.subscribe() 方法订阅主题,还可直接订阅某些主题的指定分区; </p>
<ul>
<li><p>在 KafkaConsumer 中提供了 assign() 方法来实现这些功能,此方法的具体定义如下: </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">assign</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span>;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这个方法只接受参数 partitions,用来指定需要订阅的分区集合。</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">consumer.assign(Arrays.asList(<span class="keyword">new</span> <span class="title class_">TopicPartition</span> (<span class="string">&quot;tpc_1&quot;</span> , <span class="number">0</span>),<span class="keyword">new</span> <span class="title class_">TopicPartition</span>(“tpc_2”,<span class="number">1</span>))) ;</span><br></pre></td></tr></table></figure></li>
</ul>
<h5 id="4-4-subscribe-与-assign-的区别"><a href="#4-4-subscribe-与-assign-的区别" class="headerlink" title="4.4  subscribe 与 assign 的区别"></a>4.4  subscribe 与 assign 的区别</h5><ul>
<li><p>通过 subscribe()方法订阅主题具有消费者自动再均衡功能 ; </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在多个消费者的情况下可以根据分区分配策略来自动分配各个消费者与分区的关系。 当消费组的消费者增加或减少时,分区分配关系会自动调整,以实现消费负载均衡及故障自动转移。</span><br></pre></td></tr></table></figure></li>
<li><p>assign() 方法订阅分区时,是不具备消费者自动均衡的功能的; </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">其实这一点从 assign()方法参数可以看出端倪,两种类型 subscribe()都有 ConsumerRebalanceListener 类型参数的方法,而 assign()方法却没有。</span><br></pre></td></tr></table></figure></li>
</ul>
<h5 id="4-5-取消订阅"><a href="#4-5-取消订阅" class="headerlink" title="4.5  取消订阅"></a>4.5  取消订阅</h5><p>可以使用 KafkaConsumer 中的 unsubscribe()方法采取消主题的订阅,这个方法既可以取消通过subscribe( Collection)方式实现的订阅; 也可以取消通过 subscribe(Pattem)方式实现的订阅,还可以取消通过 assign( Collection)方式实现的订阅。示例码如下: </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">consumer.unsubscribe(); </span><br></pre></td></tr></table></figure>

<p>如果将 subscribe(Collection )或 assign(Collection)集合参数设置为空集合,作用与 unsubscribe()方法相同,如下示例中三行代码的效果相同: </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">consumer.unsubscribe(); </span><br><span class="line">consumer.subscribe(<span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;String&gt;()) ; </span><br><span class="line">consumer.assign(<span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;TopicPartition&gt;());</span><br></pre></td></tr></table></figure>



<h5 id="4-6-消息的消费模式"><a href="#4-6-消息的消费模式" class="headerlink" title="4.6 消息的消费模式"></a>4.6 消息的消费模式</h5><p>Kafka 中的消费是基于拉取模式的。消息的消费一般有两种模式:推送模式和拉取模式。推模式是服务端主动将消息推送给消费者,而拉模式是消费者主动向服务端发起请求来拉取消息。</p>
<p>对于 poll () 方法而言,如果某些分区中没有可供消费的消息,那么此分区对应的消息拉取的结果就为空如果订阅的所有分区中都没有可供消费的消息,那么 poll()方法返回为空的消息集; poll () 方法具体定义如下:<br>public ConsumerRecords&lt;K, V&gt; poll(final Duration timeout)<br>超时时间参数 timeout , 用来控制 poll() 方法的阻塞时间, 在消费者的缓冲区里没有可用数据时会发生阻塞。如果消费者程序只用来单纯拉取并消费数据,则为了提高吞吐率,可以把 timeout 设置为Long.MAX_VALUE;</p>
<ul>
<li><p>消费者消费到的每条消息的类型为 ConsumerRecord</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ConsumerRecord</span>&lt;K, V&gt; &#123; </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">NO_TIMESTAMP</span> <span class="operator">=</span> RecordBatch.NO_TIMESTAMP; </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">NULL_SIZE</span> <span class="operator">=</span> -<span class="number">1</span>; </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">NULL_CHECKSUM</span> <span class="operator">=</span> -<span class="number">1</span>; </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> String topic; </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="type">int</span> partition; </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="type">long</span> offset;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="type">long</span> timestamp; </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> TimestampType timestampType; </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="type">int</span> serializedKeySize; </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="type">int</span> serializedValueSize; </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> Headers headers; </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> K key; </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> V value; </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">volatile</span> Long checksum;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">topic partition 这两个字段分别代表消息所属主题的名称和所在分区的编号。</span><br><span class="line"></span><br><span class="line">offsset 表示消息在所属分区的偏移量。</span><br><span class="line"></span><br><span class="line">timestamp 表示时间戳,与此对应的 timestampType 表示时间戳的类型。</span><br><span class="line"></span><br><span class="line">timestampType 有两种类型 CreateTime 和 LogAppendTime , 分别代表消息创建的时间戳和消息追加到日志的时间戳。</span><br><span class="line"></span><br><span class="line">headers 表示消息的头部内容。</span><br><span class="line"></span><br><span class="line">key value 分别表示消息的键和消息的值,一般业务应用要读取的就是 value ; </span><br><span class="line"></span><br><span class="line">serializedKeySize、serializedValueSize 分别表示 key、value 经过序列化之后的大小,如果 key 为空, 则 serializedKeySize 值为 -1,同样,如果 value 为空,则 serializedValueSize 的值也会为 -1; </span><br><span class="line"></span><br><span class="line">checksum 是 CRC32 的校验值。</span><br></pre></td></tr></table></figure></li>
</ul>
<h5 id="4-7-指定位移消费"><a href="#4-7-指定位移消费" class="headerlink" title="4.7 指定位移消费"></a>4.7 指定位移消费</h5><p>有些时候,我们需要一种更细粒度的掌控,可以让我们从特定的位移处开始拉取消息,而KafkaConsumer 中的 seek() 方法正好提供了这个功能,让我们可以追前消费或回溯消费。</p>
<ul>
<li><p>seek()方法的具体定义如下: </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">seek</span><span class="params">(TopicPartiton partition,<span class="type">long</span> offset)</span>;</span><br></pre></td></tr></table></figure></li>
</ul>
<h5 id="4-8-再均衡监听器"><a href="#4-8-再均衡监听器" class="headerlink" title="4.8 再均衡监听器"></a>4.8 再均衡监听器</h5><p>一个消费组中,一旦有消费者的增减发生,会触发消费者组的 rebalance 再均衡; 如果 A 消费者消费掉的一批消息还没来得及提交 offset, 而它所负责的分区在 rebalance 中转移给了 B 消费者,则有可能发生数据的重复消费处理。此情形下,可以通过再均衡监听器做一定程度的补救;</p>
<h5 id="4-9-自动位移提交"><a href="#4-9-自动位移提交" class="headerlink" title="4.9 自动位移提交"></a>4.9 自动位移提交</h5><p>Kafka 中默认的消费位移的提交方式是自动提交,这个由消费者客户端参数 enable.auto.commit 配置, 默认值为 true 。当然这个默认的自动提交不是每消费一条消息就提交一次,而是定期提交,这个定期的周期时间由客户端参数 auto.commit.interval.ms 配置, 默认值为 5 秒, 此参数生效的前提是 enable.</p>
<p>auto.commit 参数为 true。</p>
<p>在默认的方式下,消费者每隔 5 秒会将拉取到的每个分区中最大的消息位移进行提交。自动位移提交的动作是在 poll() 方法的逻辑里完成的,在每次真正向服务端发起拉取请求之前会检查是否可以进行位移提交,如果可以,那么就会提交上一次轮询的位移。</p>
<p>Kafka 消费的编程逻辑中位移提交是一大难点,自动提交消费位移的方式非常简便,它免去了复杂的位移提交逻辑,让编码更简洁。但随之而来的是重复消费和消息丢失的问题。</p>
<ul>
<li><p>重复消费</p>
<p>假设刚刚提交完一次消费位移,然后拉取一批消息进行消费,在下一次自动提交消费位移之前,消费者崩溃了,那么又得从上一次位移提交的地方重新开始消费,这样便发生了重复消费的现象(对于再均衡的情况同样适用)。我们可以通过减小位移提交的时间间隔来减小重复消息的窗口大小,但这样并不能避免重复消费的发送,而且也会使位移提交更加频繁。</p>
</li>
<li><p>丢失消息</p>
<p>按照一般思维逻辑而言,自动提交是延时提交,重复消费可以理解,那么消息丢失又是在什么情形下会发生的呢?我们来看下图中的情形: 拉取线程不断地拉取消息并存入本地缓存, 比如在 BlockingQueue 中, 另一个处理线程从缓存中读取消息并进行相应的逻辑处理。设目前进行到了第 y+l 次拉取,以及第 m 次位移提交的时候,也就是x+6 之前的位移己经确认提交了, 处理线程却还正在处理 x+3 的消息; 此时如果处理线程发生了异常, 待其恢复之后会从第 m 次位移提交处,也就是 x+6 的位置开始拉取消息,那么 x+3 至 x+6 之间的消息就没有得到相应的处理,这样便发生消息丢失的现象。</p>
</li>
</ul>
<h5 id="4-10-手动位移提交-调用-kafka-api"><a href="#4-10-手动位移提交-调用-kafka-api" class="headerlink" title="4.10 手动位移提交(调用 kafka api)"></a>4.10 手动位移提交(调用 kafka api)</h5><p>自动位移提交的方式在正常情况下不会发生消息丢失或重复消费的现象, 但是在编程的世界里异常无可避免; 同时, 自动位移提交也无法做到精确的位移管理。 在 Kafka 中还提供了手动位移提交的方式, 这样可以使得开发人员对消费位移的管理控制更加灵活。<br>很多时候并不是说拉取到消息就算消费完成,而是需要将消息写入数据库、写入本地缓存,或者是更加复杂的业务处理。在这些场景下,所有的业务处理完成才能认为消息被成功消费; 手动的提交方式可以让开发人员根据程序的逻辑在合适的地方进行位移提交。 开启手动提交功能的前提是消费者客户端参数 enable.auto.commit 配置为 fals ,示例如下:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">props.put(ConsumerConf.ENABLE_AUTO_COMMIT_CONFIG, <span class="literal">false</span>); </span><br></pre></td></tr></table></figure>

<p>手动提交可以细分为同步提交和异步提交,对应于 KafkaConsumer 中的 commitSync()和commitAsync()两种类型的方法。</p>
<h4 id="5、Topic管理-API"><a href="#5、Topic管理-API" class="headerlink" title="5、Topic管理 API"></a>5、Topic管理 API</h4><p>一般情况下,我们都习惯使用 kafka-topic.sh 本来管理主题,如果希望将管理类的功能集成到公司内部的系统中,打造集管理、监控、运维、告警为一体的生态平台,那么就需要以程序调用 API 方式去实现。这种调用 API 方式实现管理主要利用 KafkaAdminClient 工具类KafkaAdminClient 不仅可以用来管理 broker、配置和 ACL (Access Control List),还可用来管理主题)</p>
<h5 id="5-1-列出主题"><a href="#5-1-列出主题" class="headerlink" title="5.1 列出主题"></a>5.1 列出主题</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">ListTopicsResult</span> <span class="variable">listTopicsResult</span> <span class="operator">=</span> adminClient.listTopics(); </span><br><span class="line"></span><br><span class="line">Set&lt;String&gt; topics = listTopicsResult.names().get(); </span><br><span class="line"></span><br><span class="line">System.out.println(topics);</span><br></pre></td></tr></table></figure>



<h5 id="5-2-查看主题信息"><a href="#5-2-查看主题信息" class="headerlink" title="5.2 查看主题信息"></a>5.2 查看主题信息</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">DescribeTopicsResult</span> <span class="variable">describeTopicsResult</span> <span class="operator">=</span> adminClient.describeTopics(Arrays.asList(<span class="string">&quot;tpc_4&quot;</span>, <span class="string">&quot;tpc_3&quot;</span>)); </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Map&lt;String, TopicDescription&gt; res = describeTopicsResult.all().get();</span><br><span class="line"></span><br><span class="line">Set&lt;String&gt; ksets = res.keySet(); </span><br><span class="line"><span class="keyword">for</span> (String k : ksets) &#123; </span><br><span class="line">	System.out.println(res.get(k)); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h5 id="5-3-创建主题"><a href="#5-3-创建主题" class="headerlink" title="5.3 创建主题"></a>5.3 创建主题</h5><ul>
<li>代码示例（部分截取）</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 参数配置</span></span><br><span class="line"><span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>(); </span><br><span class="line">props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG,<span class="string">&quot;master:9092,slave1:9092,slave2:9092&quot;</span>); </span><br><span class="line">props.put(AdminClientConfig.REQUEST_TIMEOUT_MS_CONFIG,<span class="number">3000</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建 admin client 对象</span></span><br><span class="line"><span class="type">AdminClient</span> <span class="variable">adminClient</span> <span class="operator">=</span> KafkaAdminClient.create(props); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 由服务端 controller 自行分配分区及副本所在 broker </span></span><br><span class="line"><span class="type">NewTopic</span> <span class="variable">tpc_3</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">NewTopic</span>(<span class="string">&quot;tpc_3&quot;</span>, <span class="number">2</span>, (<span class="type">short</span>) <span class="number">1</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 手动指定分区及副本的 broker 分配</span></span><br><span class="line">HashMap&lt;Integer, List&lt;Integer&gt;&gt; replicaAssignments = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;(); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 分区 0,分配到 broker0,broker1 replicaAssignments.put(0,Arrays.asList(0,1)); </span></span><br><span class="line"><span class="comment">// 分区 1,分配到 broker0,broker2 </span></span><br><span class="line">replicaAssignments.put(<span class="number">0</span>,Arrays.asList(<span class="number">0</span>,<span class="number">1</span>));</span><br><span class="line"><span class="type">NewTopic</span> <span class="variable">tpc_4</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">NewTopic</span>(<span class="string">&quot;tpc_4&quot;</span>, replicaAssignments); </span><br><span class="line"><span class="type">CreateTopicsResult</span> <span class="variable">result</span> <span class="operator">=</span> adminClient.createTopics(Arrays.asList(tpc_3,tpc_4)); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 从 future 中等待服务端返回</span></span><br><span class="line"><span class="keyword">try</span> &#123; </span><br><span class="line">	result.all().get(); </span><br><span class="line">&#125; <span class="keyword">catch</span> (Exception e) &#123; </span><br><span class="line">e.printStackTrace(); </span><br><span class="line">&#125; </span><br><span class="line">adminClient.close();</span><br></pre></td></tr></table></figure>



<h5 id="5-4-删除主题"><a href="#5-4-删除主题" class="headerlink" title="5.4 删除主题"></a>5.4 删除主题</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">DeleteTopicsResult</span> <span class="variable">deleteTopicsResult</span> <span class="operator">=</span> adminClient.deleteTopics(Arrays.asList(<span class="string">&quot;tpc_1&quot;</span>, <span class="string">&quot;tpc_1&quot;</span>)); </span><br><span class="line"></span><br><span class="line">Map&lt;String, KafkaFuture&lt;Void&gt;&gt; values = deleteTopicsResult.values();</span><br><span class="line"></span><br><span class="line">System.out.println(values);</span><br></pre></td></tr></table></figure>



<h5 id="5-5-其他管理"><a href="#5-5-其他管理" class="headerlink" title="5.5 其他管理"></a>5.5 其他管理</h5><p>除了进行 topic 管理之外,KafkaAdminClient 也可以进行诸如动态参数管理,分区管理等各类管理操作;</p>

</article>

    <div class="pagenator post-pagenator">
    
    

    
    <p>上次更新 2022-06-05</p>
    
    
        <a class="extend next post-next" href="/2022/06/05/%E4%BA%8C%E3%80%81Kafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/">下一篇</a>
    
    </div>


    </div>
    <div class="footer">
        <div class="container">
    <div class="social">
	<ul class="social-list">
		
			
				
				
				<li>
					<a href="mailto:NoahRicardo@qq.com" title="email" target="_blank">
					<i class="fa fa-email"></i>
					</a>
				</li>
			
		
			
		
			
		
			
		
			
		
			
		
			
				
				<li>
					<a href="https://github.com/NoahRicardo/" title="github" target="_self">
					<i class="fa fa-github"></i>
					</a>
				</li>
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
	</ul>
</div>
    <div class="copyright">
        <span>
            
            
            
                © NoahRicardo 2017 - 2022
            
        </span>
    </div>
    <div class="power">
        <span>
            Powered by <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a> & <a target="_blank" rel="noopener" href="https://github.com/CaiChenghan/iLiKE">iLiKE Theme</a>
        </span>
    </div>
    <script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.js"></script>
    <!--page counter part-->
<script>
function addCount (Counter) {
    url=$('.article-date').attr('href').trim();
    title = $('.article-title').text().trim();
    var query = new AV.Query(Counter);
    //use url as unique idnetfication
    query.equalTo("url",url);
    query.find({
        success: function(results) {
            if (results.length>0) {
                var counter=results[0];
                counter.fetchWhenSave(true); //get recent result
                counter.increment("time");
                counter.save();
            } else {
                var newcounter=new Counter();
                newcounter.set("title",title);
                newcounter.set("url",url);
                newcounter.set("time",1);
                newcounter.save(null,{
                    success: function(newcounter) {
                        //alert('New object created');
                    }, error: function(newcounter,error) {
                        alert('Failed to create');
                    }
                })
            }
        },
        error: function(error) {
            //find null is not a error
            alert('Error:'+error.code+" "+error.message);
        }
    });
}
$(function() {
    var Counter=AV.Object.extend("Counter");
    //only increse visit counting when intering a page
    if ($('.article-title').length == 1) {
       addCount(Counter);
    }
    var query=new AV.Query(Counter);
    query.descending("time");
    // the sum of popular posts
    query.limit(10); 
    query.find({
        success: function(results) {
                for(var i=0;i<results.length;i++) {
                    var counter=results[i];
                    title=counter.get("title");
                    url=counter.get("url");
                    time=counter.get("time");
                    // add to the popularlist widget
                    showcontent=title+" ("+time+")";
                    //notice the "" in href
                    $('.popularlist').append('<li><a href="'+url+'">'+showcontent+'</a></li>');
                }
            },
        error: function(error) {
            alert("Error:"+error.code+" "+error.message);
        }
    });
});
</script>
</div>
    </div>
</body>
</html>
